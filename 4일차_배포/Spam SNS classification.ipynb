{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spam SNS classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Vy_G2wCjua9","executionInfo":{"status":"ok","timestamp":1657521752595,"user_tz":-540,"elapsed":2552,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"cd5f08b6-7e1f-4184-d016-a37852a4ab47"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2Eq1SjUWGER","executionInfo":{"status":"ok","timestamp":1657521755502,"user_tz":-540,"elapsed":2910,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"06606f05-2e0f-4c99-e16a-bb1da882f5c6"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"]}]},{"cell_type":"markdown","source":["# Spam SNS classification"],"metadata":{"id":"RjVOZjEU2pan"}},{"cell_type":"markdown","source":["각 문장이 spam 메일인지 아닌지를 판별하는 문장 분류 태스크  \n","dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset  \n","4000개까지 train으로 사용  \n","\n","Dataset 구축  \n","dataset에서는 __ getitem __을 통해 한개의 데이터를 모델의 입력형태에 맞추어 반환한다.  \n","이번 코드에서는 자연어 문장을 모델에 입력하기 위해 tokenization과 vocab dictionary에 따른 index로의 변환을 진행한다.  \n","또한 label의 ham/spam에 따라 0/1을 label로 변환한다.  \n","\n","주요 class, method:  \n","torchtext.data.utils.get_tokenizer: torchtext에서 제공하는 tokenizer 문법에 따른 tokenization을 수행하는 class 반환  \n","torchtext.vocab.build_vocab_from_iterator:  내 학습데이터에 대한 모든 단어를 입력하면 각 단어에 한개씩 index를 부여한 vocab dictionary 반환  \n","**huggingface.tokenizer:** 사전에 학습된 tokenization을 불러오는 huggingface class  \n","https://huggingface.co/docs/transformers/main_classes/tokenizer\n"],"metadata":{"id":"Hu1IWE_L2xM3"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer,BertTokenizer\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","#dataset load\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/Natural_Language_Processing/4일차_배포/dataset/spam.csv\"\n","data_df = pd.read_csv(data_path, encoding = \"ISO-8859-1\")\n","print(data_df)\n","print(data_df.columns)\n","\n","#train test split\n","data_df = data_df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n","train_df = data_df.loc[:200,:].reset_index()\n","test_df = data_df.loc[4000:,:].reset_index()\n","print(train_df)\n","print(train_df.columns)\n","\n","#tokenizer load\n","tokenizer=get_tokenizer(\"basic_english\")\n","d = data_df.loc[0:3,\"v2\"]\n","print(d)\n","print(tokenizer(data_df.loc[0,\"v2\"]))\n","\n","#build vocab dictionary\n","vocab = build_vocab_from_iterator(list(map(tokenizer,data_df.loc[:,\"v2\"])))\n","print(vocab(tokenizer(data_df.loc[0,\"v2\"])))\n","\n","#huggingface tokenizer load\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","print(tokenizer(data_df.loc[0,\"v2\"], max_length=100, padding='max_length', truncation=True))\n","\n","class myDataset(Dataset):\n","  def __init__(self, df, tokenizer, vocab) -> None:\n","      super().__init__()\n","      self.df = df\n","      self.tokenizer = tokenizer\n","      self.vocab = vocab\n","\n","  def __len__(self):\n","      return len(self.df)\n","  \n","  def __getitem__(self, index):\n","      data = self.df.loc[index, \"v2\"]\n","      target = self.df.loc[index, \"v1\"]\n","      \n","      #data tokenization\n","      data = self.tokenizer(data, max_length=100,padding='max_length', truncation=True)[\"input_ids\"]\n","\n","      data.reverse() # 역순으로\n","      # 왜냐면 vanishing gradient problem(시간이 지날수록 단어의 정보가 소실되는 문제)를 줄이기 위해 역순으로 만들어버림\n","      # max_length를 맞추기 위해 마지막에 0으로 다 채워주는데 역순으로 해주면 0 들이 처음에 나와서 그런 문제 줄일 수 있음\n","\n","      #labeling\n","      if target == \"ham\":\n","          label = 0\n","      elif target == \"spam\":\n","          label = 1\n","\n","      return torch.tensor(data, dtype=torch.int64), label\n","\n","\n","train_dataset = myDataset(train_df, tokenizer, vocab)\n","test_dataset = myDataset(test_df, tokenizer, vocab)\n","batch_size = 100\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size)\n","\n","for i in train_dataset:\n","  print(i)\n","  break\n","  \n","for i in train_dataloader:\n","  print(i)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kWdTTr8XWal","executionInfo":{"status":"ok","timestamp":1657521756114,"user_tz":-540,"elapsed":616,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"03571359-e4ef-4207-9656-9518bac0a70c"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["        v1                                                 v2 Unnamed: 2  \\\n","0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n","1      ham                      Ok lar... Joking wif u oni...        NaN   \n","2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n","3      ham  U dun say so early hor... U c already then say...        NaN   \n","4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n","...    ...                                                ...        ...   \n","5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n","5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n","5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n","5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n","5571   ham                         Rofl. Its true to its name        NaN   \n","\n","     Unnamed: 3 Unnamed: 4  \n","0           NaN        NaN  \n","1           NaN        NaN  \n","2           NaN        NaN  \n","3           NaN        NaN  \n","4           NaN        NaN  \n","...         ...        ...  \n","5567        NaN        NaN  \n","5568        NaN        NaN  \n","5569        NaN        NaN  \n","5570        NaN        NaN  \n","5571        NaN        NaN  \n","\n","[5572 rows x 5 columns]\n","Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')\n","     index    v1                                                 v2\n","0        0   ham  Go until jurong point, crazy.. Available only ...\n","1        1   ham                      Ok lar... Joking wif u oni...\n","2        2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3        3   ham  U dun say so early hor... U c already then say...\n","4        4   ham  Nah I don't think he goes to usf, he lives aro...\n","..     ...   ...                                                ...\n","196    196   ham                       Did u got that persons story\n","197    197   ham  is your hamster dead? Hey so tmr i meet you at...\n","198    198   ham  Hi its Kate how is your evening? I hope i can ...\n","199    199   ham           Found it, ENC  &lt;#&gt; , where you at?\n","200    200   ham                       I sent you  &lt;#&gt;  bucks\n","\n","[201 rows x 3 columns]\n","Index(['index', 'v1', 'v2'], dtype='object')\n","0    Go until jurong point, crazy.. Available only ...\n","1                        Ok lar... Joking wif u oni...\n","2    Free entry in 2 a wkly comp to win FA Cup fina...\n","3    U dun say so early hor... U c already then say...\n","Name: v2, dtype: object\n","['go', 'until', 'jurong', 'point', ',', 'crazy', '.', '.', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '.', '.', '.', 'cine', 'there', 'got', 'amore', 'wat', '.', '.', '.']\n","[56, 471, 6957, 862, 4, 782, 0, 0, 651, 75, 12, 1321, 100, 131, 367, 1375, 169, 3339, 0, 0, 0, 1328, 67, 66, 5229, 143, 0, 0, 0]\n","{'input_ids': [101, 3414, 1235, 179, 22497, 1403, 1553, 117, 4523, 119, 119, 11651, 8009, 2165, 1178, 1107, 15430, 1548, 183, 1632, 1362, 2495, 174, 171, 9435, 2105, 119, 119, 119, 140, 2042, 1175, 1400, 1821, 4474, 20049, 1204, 119, 119, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","(tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,   102,\n","          119,   119,   119,  1204, 20049,  4474,  1821,  1400,  1175,  2042,\n","          140,   119,   119,   119,  2105,  9435,   171,   174,  2495,  1362,\n","         1632,   183,  1548, 15430,  1107,  1178,  2165,  8009, 11651,   119,\n","          119,  4523,   117,  1553,  1403, 22497,   179,  1235,  3414,   101]), 0)\n","[tensor([[    0,     0,     0,  ...,  1235,  3414,   101],\n","        [    0,     0,     0,  ...,  2495, 23330,   101],\n","        [    0,     0,     0,  ...,  3990,  4299,   101],\n","        ...,\n","        [    0,     0,     0,  ...,  1267,   178,   101],\n","        [    0,     0,     0,  ...,   119,  8790,   101],\n","        [    0,     0,     0,  ...,  1274,  4203,   101]]), tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n","        0, 0, 0, 0])]\n"]}]},{"cell_type":"markdown","source":["model 선언  \n","문장을 RNN/LSTM을 통해 classification하는 model 선언\n","1. token의 index를 입력으로 받고 word embedding을 결과로 반환하는 nn.Embedding 사용  \n","2. LSTM을 통해 모든 token을 입력  \n","3. many-to-one구조를 가지기 때문에 LSTM의 결과중 마지막 cell에 대한 결과만을 사용하여 nn.linear를통해 classification\n","\n","입력으로는 문장을 tokenziation과 indexing한게 입력으로 들어오기 때문에(batch, sequance length)형태의 입력  \n","이후 nn.Embedding을 거치면서 각 단어의 벡터가 생성되기 때문에(batch, sequance length, hidden size)의 형태 사용\n","\n","주요 obejct:  \n","**nn.Embedding:**https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html  \n","**nn.LSTM:**https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html  \n","**nn.RNN:**https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"],"metadata":{"id":"xfKV9_575XYT"}},{"cell_type":"code","source":["from torch import nn\n","\n","class myModel(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.emb = nn.Embedding(50000,128)\n","        self.lstm = nn.LSTM(128, 128, batch_first = True)\n","        self.linaer = nn.Linear(128, 2)\n","\n","    def forward(self, x):\n","\n","        x = self.emb(x)\n","        x, _ = self.lstm(x)\n","        \n","        #마지막 time(seq length)에 대한 결과만을 사용하여 classification\n","        x = self.linaer(x[:,-1,:])\n","\n","        return x\n","\n","model = myModel()\n","\n","for i in train_dataloader:\n","  data = i[0]\n","  label = i[1]\n","  data = model(data)\n","  print(data)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss1jJefWEw6X","executionInfo":{"status":"ok","timestamp":1657521756114,"user_tz":-540,"elapsed":4,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"a2e143ee-d3a8-49ce-fe99-c8e451cde84f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1304,  0.0584],\n","        [-0.1050, -0.0757],\n","        [-0.1443,  0.0327],\n","        [-0.0120,  0.0172],\n","        [-0.0038, -0.0440],\n","        [-0.0414,  0.0437],\n","        [-0.0651,  0.0106],\n","        [-0.0475,  0.0526],\n","        [-0.0087,  0.0048],\n","        [-0.0040, -0.0141],\n","        [-0.0371, -0.0163],\n","        [-0.0723,  0.0139],\n","        [-0.0083,  0.0096],\n","        [-0.0089, -0.0292],\n","        [-0.0743, -0.0130],\n","        [-0.1219,  0.0372],\n","        [-0.0821,  0.0324],\n","        [-0.0475,  0.0059],\n","        [-0.0259,  0.0324],\n","        [-0.0147, -0.0646],\n","        [-0.0650,  0.0426],\n","        [-0.0280,  0.0451],\n","        [-0.0349,  0.0519],\n","        [-0.0679, -0.0488],\n","        [-0.0553, -0.0316],\n","        [-0.0934, -0.0247],\n","        [-0.1194, -0.0531],\n","        [-0.0444,  0.0312],\n","        [-0.0209, -0.0065],\n","        [-0.0726, -0.0718],\n","        [-0.1751,  0.0332],\n","        [-0.1053, -0.0255],\n","        [-0.0105,  0.0188],\n","        [-0.1062,  0.0345],\n","        [-0.0368, -0.0026],\n","        [-0.1385,  0.0110],\n","        [-0.0348,  0.0247],\n","        [-0.0420,  0.0209],\n","        [-0.0603,  0.0192],\n","        [-0.1148,  0.0796],\n","        [-0.0322,  0.0225],\n","        [ 0.0218,  0.0305],\n","        [-0.0550, -0.0662],\n","        [-0.0650, -0.0116],\n","        [-0.0757, -0.0623],\n","        [-0.0566, -0.0169],\n","        [-0.0299, -0.0221],\n","        [-0.0330, -0.0087],\n","        [-0.0755,  0.0017],\n","        [ 0.0190, -0.0082],\n","        [ 0.0208, -0.0317],\n","        [-0.0556, -0.0329],\n","        [-0.0475, -0.0047],\n","        [-0.0389, -0.0025],\n","        [-0.0295, -0.0312],\n","        [-0.1105,  0.0246],\n","        [-0.0166,  0.0023],\n","        [-0.0695,  0.0400],\n","        [ 0.0116,  0.0608],\n","        [-0.0948,  0.0128],\n","        [-0.0653, -0.0137],\n","        [-0.0999, -0.0320],\n","        [-0.1537,  0.0381],\n","        [-0.1139,  0.0780],\n","        [-0.1024, -0.0649],\n","        [-0.0488,  0.0671],\n","        [-0.0107, -0.0524],\n","        [-0.0019,  0.0457],\n","        [-0.0058,  0.0275],\n","        [ 0.0067,  0.0150],\n","        [-0.0044, -0.0124],\n","        [-0.0305, -0.0657],\n","        [-0.0539, -0.0269],\n","        [-0.0403,  0.0057],\n","        [ 0.0118, -0.0132],\n","        [-0.0427, -0.0132],\n","        [-0.0427,  0.0352],\n","        [-0.0227,  0.0087],\n","        [-0.0269,  0.0614],\n","        [-0.1445,  0.0610],\n","        [-0.0647,  0.0423],\n","        [-0.0345,  0.0207],\n","        [-0.0859, -0.0558],\n","        [-0.0201,  0.0394],\n","        [-0.1196,  0.0490],\n","        [-0.0020,  0.0020],\n","        [-0.0480,  0.0416],\n","        [-0.0308,  0.0088],\n","        [-0.0161, -0.0106],\n","        [-0.0109, -0.0777],\n","        [-0.0802,  0.0019],\n","        [-0.0806,  0.0971],\n","        [-0.0736,  0.0319],\n","        [-0.0471, -0.0118],\n","        [-0.0806,  0.0204],\n","        [-0.0671, -0.0085],\n","        [-0.1060,  0.0055],\n","        [-0.0641,  0.0153],\n","        [-0.0310,  0.0628],\n","        [-0.0540, -0.0427]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["model train"],"metadata":{"id":"sg8IRfDM7Z_3"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","model = myModel()\n","model.cuda()\n","\n","#학습을 위한 optimizer와 loss function 설정\n","optimizer = Adam(model.parameters(), lr=0.001)\n","lf = CrossEntropyLoss().cuda()\n","\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  print(\"\\n\\nepoch \", e)\n","  epoch_loss = 0\n","  train_correct = 0 \n","  \n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in train_dataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","    data = i[0]\n","    data = data.cuda()\n","    target = i[1]\n","    target = target.cuda()\n","\n","    #결과 도출 및 정답수 연산\n","    output = model(data)\n","    pred_label = torch.argmax(output, dim=-1)\n","    train_correct += sum(pred_label == target.reshape(-1))\n","\n","    target = target.reshape(-1)\n","    #loss연산\n","    loss = lf(output, target)\n","    #print(loss)\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  print(train_correct)\n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","  print(\"train acc\", train_correct/len(train_dataset))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","  test_correct = 0 \n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in test_dataloader:\n","      data = i[0]\n","      target = i[1]\n","      data = data.cuda()\n","      target = target.cuda()\n","\n","      output = model(data)\n","\n","      loss = lf(output, target.reshape(-1))\n","      pred_label = torch.argmax(output, dim=-1)\n","      test_correct += sum(pred_label == target.reshape(-1))\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(test_dataloader))\n","  print(\"test acc\", test_correct/len(test_dataset))\n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmVPBc1AN4j_","executionInfo":{"status":"ok","timestamp":1657521862661,"user_tz":-540,"elapsed":106549,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"1a14ed4e-b336-4732-dcc9-60185ba22c77"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","epoch  0\n","tensor(119, device='cuda:0')\n","train loss 0.5915282666683197\n","train acc tensor(0.5920, device='cuda:0')\n","test loss 0.4709295444190502\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  1\n","tensor(168, device='cuda:0')\n","train loss 0.3748283237218857\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.388747476041317\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  2\n","tensor(168, device='cuda:0')\n","train loss 0.3072759558757146\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.38182023726403713\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  3\n","tensor(168, device='cuda:0')\n","train loss 0.3013172708451748\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3919399678707123\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  4\n","tensor(168, device='cuda:0')\n","train loss 0.3041407708078623\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3924777191132307\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  5\n","tensor(168, device='cuda:0')\n","train loss 0.29628856976826984\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3809600453823805\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  6\n","tensor(168, device='cuda:0')\n","train loss 0.2772189707805713\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.3628262896090746\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  7\n","tensor(168, device='cuda:0')\n","train loss 0.251606870132188\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.34343694150447845\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  8\n","tensor(168, device='cuda:0')\n","train loss 0.2234314704934756\n","train acc tensor(0.8358, device='cuda:0')\n","test loss 0.32459103502333164\n","test acc tensor(0.8651, device='cuda:0')\n","\n","\n","epoch  9\n","tensor(170, device='cuda:0')\n","train loss 0.19344819399217764\n","train acc tensor(0.8458, device='cuda:0')\n","test loss 0.3038029130548239\n","test acc tensor(0.8728, device='cuda:0')\n","\n","\n","epoch  10\n","tensor(172, device='cuda:0')\n","train loss 0.16145010944455862\n","train acc tensor(0.8557, device='cuda:0')\n","test loss 0.28662166744470596\n","test acc tensor(0.8785, device='cuda:0')\n","\n","\n","epoch  11\n","tensor(176, device='cuda:0')\n","train loss 0.13246111292392015\n","train acc tensor(0.8756, device='cuda:0')\n","test loss 0.2633913066238165\n","test acc tensor(0.8906, device='cuda:0')\n","\n","\n","epoch  12\n","tensor(192, device='cuda:0')\n","train loss 0.09785941382870078\n","train acc tensor(0.9552, device='cuda:0')\n","test loss 0.23229760956019163\n","test acc tensor(0.9046, device='cuda:0')\n","\n","\n","epoch  13\n","tensor(197, device='cuda:0')\n","train loss 0.06827227891578029\n","train acc tensor(0.9801, device='cuda:0')\n","test loss 0.2270749374292791\n","test acc tensor(0.9090, device='cuda:0')\n","\n","\n","epoch  14\n","tensor(199, device='cuda:0')\n","train loss 0.04988292387376229\n","train acc tensor(0.9900, device='cuda:0')\n","test loss 0.28040670696645975\n","test acc tensor(0.9109, device='cuda:0')\n","\n","\n","epoch  15\n","tensor(199, device='cuda:0')\n","train loss 0.04329989186953753\n","train acc tensor(0.9900, device='cuda:0')\n","test loss 0.22369752638041973\n","test acc tensor(0.8957, device='cuda:0')\n","\n","\n","epoch  16\n","tensor(199, device='cuda:0')\n","train loss 0.04770584939979017\n","train acc tensor(0.9900, device='cuda:0')\n","test loss 0.22745411843061447\n","test acc tensor(0.8957, device='cuda:0')\n","\n","\n","epoch  17\n","tensor(201, device='cuda:0')\n","train loss 0.032671296095941216\n","train acc tensor(1., device='cuda:0')\n","test loss 0.22722376976162195\n","test acc tensor(0.9160, device='cuda:0')\n","\n","\n","epoch  18\n","tensor(201, device='cuda:0')\n","train loss 0.01660951281276842\n","train acc tensor(1., device='cuda:0')\n","test loss 0.29065502528101206\n","test acc tensor(0.9173, device='cuda:0')\n","\n","\n","epoch  19\n","tensor(201, device='cuda:0')\n","train loss 0.016627509011110913\n","train acc tensor(1., device='cuda:0')\n","test loss 0.24140553455799818\n","test acc tensor(0.9224, device='cuda:0')\n","\n","\n","epoch  20\n","tensor(201, device='cuda:0')\n","train loss 0.00981998274801299\n","train acc tensor(1., device='cuda:0')\n","test loss 0.23162641655653715\n","test acc tensor(0.9154, device='cuda:0')\n","\n","\n","epoch  21\n","tensor(201, device='cuda:0')\n","train loss 0.008760943475256985\n","train acc tensor(1., device='cuda:0')\n","test loss 0.23657089937478304\n","test acc tensor(0.9097, device='cuda:0')\n","\n","\n","epoch  22\n","tensor(201, device='cuda:0')\n","train loss 0.0076412880832018954\n","train acc tensor(1., device='cuda:0')\n","test loss 0.24203103315085173\n","test acc tensor(0.9065, device='cuda:0')\n","\n","\n","epoch  23\n","tensor(201, device='cuda:0')\n","train loss 0.006296229645765076\n","train acc tensor(1., device='cuda:0')\n","test loss 0.245402074418962\n","test acc tensor(0.9084, device='cuda:0')\n","\n","\n","epoch  24\n","tensor(201, device='cuda:0')\n","train loss 0.004965086147421971\n","train acc tensor(1., device='cuda:0')\n","test loss 0.24787938315421343\n","test acc tensor(0.9109, device='cuda:0')\n","\n","\n","epoch  25\n","tensor(201, device='cuda:0')\n","train loss 0.0038953017986690006\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2510634884238243\n","test acc tensor(0.9167, device='cuda:0')\n","\n","\n","epoch  26\n","tensor(201, device='cuda:0')\n","train loss 0.003126356100741153\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2561375955119729\n","test acc tensor(0.9205, device='cuda:0')\n","\n","\n","epoch  27\n","tensor(201, device='cuda:0')\n","train loss 0.0025892170718483007\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2627158113755286\n","test acc tensor(0.9205, device='cuda:0')\n","\n","\n","epoch  28\n","tensor(201, device='cuda:0')\n","train loss 0.002205533872862967\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2696015420369804\n","test acc tensor(0.9243, device='cuda:0')\n","\n","\n","epoch  29\n","tensor(201, device='cuda:0')\n","train loss 0.0019208526549239953\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2762248604558408\n","test acc tensor(0.9268, device='cuda:0')\n","\n","\n","epoch  30\n","tensor(201, device='cuda:0')\n","train loss 0.0017022026586346328\n","train acc tensor(1., device='cuda:0')\n","test loss 0.28239865321666\n","test acc tensor(0.9268, device='cuda:0')\n","\n","\n","epoch  31\n","tensor(201, device='cuda:0')\n","train loss 0.0015293887748460595\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2880183942615986\n","test acc tensor(0.9268, device='cuda:0')\n","\n","\n","epoch  32\n","tensor(201, device='cuda:0')\n","train loss 0.0013894471582413341\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2929979935288429\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  33\n","tensor(201, device='cuda:0')\n","train loss 0.0012737573609532167\n","train acc tensor(1., device='cuda:0')\n","test loss 0.2973078293725848\n","test acc tensor(0.9268, device='cuda:0')\n","\n","\n","epoch  34\n","tensor(201, device='cuda:0')\n","train loss 0.0011764369749774535\n","train acc tensor(1., device='cuda:0')\n","test loss 0.30098220612853765\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  35\n","tensor(201, device='cuda:0')\n","train loss 0.0010931943737280865\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3040969166904688\n","test acc tensor(0.9300, device='cuda:0')\n","\n","\n","epoch  36\n","tensor(201, device='cuda:0')\n","train loss 0.001021083085409676\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3067431105300784\n","test acc tensor(0.9300, device='cuda:0')\n","\n","\n","epoch  37\n","tensor(201, device='cuda:0')\n","train loss 0.0009578467240013803\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3090100912377238\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  38\n","tensor(201, device='cuda:0')\n","train loss 0.0009018690179800615\n","train acc tensor(1., device='cuda:0')\n","test loss 0.31097821053117514\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  39\n","tensor(201, device='cuda:0')\n","train loss 0.0008519245481390195\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3127162102609873\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  40\n","tensor(201, device='cuda:0')\n","train loss 0.0008069445975706913\n","train acc tensor(1., device='cuda:0')\n","test loss 0.314281202852726\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  41\n","tensor(201, device='cuda:0')\n","train loss 0.0007662690792737218\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3157192962244153\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  42\n","tensor(201, device='cuda:0')\n","train loss 0.0007292163184805153\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3170668035745621\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  43\n","tensor(201, device='cuda:0')\n","train loss 0.0006952995827305131\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3183515705168247\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  44\n","tensor(201, device='cuda:0')\n","train loss 0.0006641203629745481\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3195943059399724\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  45\n","tensor(201, device='cuda:0')\n","train loss 0.000635316920427916\n","train acc tensor(1., device='cuda:0')\n","test loss 0.32081010239198804\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  46\n","tensor(201, device='cuda:0')\n","train loss 0.0006086033778653169\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3220093008130789\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  47\n","tensor(201, device='cuda:0')\n","train loss 0.0005837891658302397\n","train acc tensor(1., device='cuda:0')\n","test loss 0.32319881906732917\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  48\n","tensor(201, device='cuda:0')\n","train loss 0.0005606407915668873\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3243827810510993\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  49\n","tensor(201, device='cuda:0')\n","train loss 0.000538949937132808\n","train acc tensor(1., device='cuda:0')\n","test loss 0.32556326035410166\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  50\n","tensor(201, device='cuda:0')\n","train loss 0.0005186519823231114\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3267408860847354\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  51\n","tensor(201, device='cuda:0')\n","train loss 0.0004995447040225068\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3279152656905353\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  52\n","tensor(201, device='cuda:0')\n","train loss 0.0004815612386058395\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3290852550417185\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  53\n","tensor(201, device='cuda:0')\n","train loss 0.00046456733980448917\n","train acc tensor(1., device='cuda:0')\n","test loss 0.330249207559973\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  54\n","tensor(201, device='cuda:0')\n","train loss 0.00044852350401924923\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3314053025096655\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  55\n","tensor(201, device='cuda:0')\n","train loss 0.00043335925874998793\n","train acc tensor(1., device='cuda:0')\n","test loss 0.33255163859575987\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  56\n","tensor(201, device='cuda:0')\n","train loss 0.0004189644387224689\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3336863317526877\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  57\n","tensor(201, device='cuda:0')\n","train loss 0.00040529003793684143\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3348078038543463\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  58\n","tensor(201, device='cuda:0')\n","train loss 0.00039231709282224375\n","train acc tensor(1., device='cuda:0')\n","test loss 0.33591459644958377\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  59\n","tensor(201, device='cuda:0')\n","train loss 0.00037996258470229805\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3370058871805668\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  60\n","tensor(201, device='cuda:0')\n","train loss 0.00036823040863964707\n","train acc tensor(1., device='cuda:0')\n","test loss 0.33808130491524935\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  61\n","tensor(201, device='cuda:0')\n","train loss 0.0003570424087229185\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3391414009965956\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  62\n","tensor(201, device='cuda:0')\n","train loss 0.0003463556131464429\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34018770372495055\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  63\n","tensor(201, device='cuda:0')\n","train loss 0.0003361840305539469\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3412230107933283\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  64\n","tensor(201, device='cuda:0')\n","train loss 0.0003264599484585536\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34225169103592634\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  65\n","tensor(201, device='cuda:0')\n","train loss 0.0003171559401380364\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34327990654855967\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  66\n","tensor(201, device='cuda:0')\n","train loss 0.00030828974801503745\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3443157817237079\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  67\n","tensor(201, device='cuda:0')\n","train loss 0.00029979633715508197\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34536918299272656\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  68\n","tensor(201, device='cuda:0')\n","train loss 0.00029165234688359004\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34645092813298106\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  69\n","tensor(201, device='cuda:0')\n","train loss 0.0002838864323469655\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3475715573877096\n","test acc tensor(0.9294, device='cuda:0')\n","\n","\n","epoch  70\n","tensor(201, device='cuda:0')\n","train loss 0.0002763934547450238\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34873903915286064\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  71\n","tensor(201, device='cuda:0')\n","train loss 0.00026924292008819367\n","train acc tensor(1., device='cuda:0')\n","test loss 0.34995744470506907\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  72\n","tensor(201, device='cuda:0')\n","train loss 0.0002623725874097242\n","train acc tensor(1., device='cuda:0')\n","test loss 0.35122613376006484\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  73\n","tensor(201, device='cuda:0')\n","train loss 0.00025576864209142514\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3525407682172954\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  74\n","tensor(201, device='cuda:0')\n","train loss 0.00024946367799808894\n","train acc tensor(1., device='cuda:0')\n","test loss 0.353894900996238\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  75\n","tensor(201, device='cuda:0')\n","train loss 0.0002433619083603844\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3552812379784882\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  76\n","tensor(201, device='cuda:0')\n","train loss 0.0002375311038728493\n","train acc tensor(1., device='cuda:0')\n","test loss 0.35669263545423746\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  77\n","tensor(201, device='cuda:0')\n","train loss 0.00023191629831368724\n","train acc tensor(1., device='cuda:0')\n","test loss 0.35812231432646513\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  78\n","tensor(201, device='cuda:0')\n","train loss 0.00022651208685905053\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3595641334541142\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  79\n","tensor(201, device='cuda:0')\n","train loss 0.00022130735305836424\n","train acc tensor(1., device='cuda:0')\n","test loss 0.36101256450638175\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  80\n","tensor(201, device='cuda:0')\n","train loss 0.00021632708618805432\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3624627785757184\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  81\n","tensor(201, device='cuda:0')\n","train loss 0.00021149272773376046\n","train acc tensor(1., device='cuda:0')\n","test loss 0.36391062242910266\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  82\n","tensor(201, device='cuda:0')\n","train loss 0.00020686903978154683\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3653528029099107\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  83\n","tensor(201, device='cuda:0')\n","train loss 0.0002024079561427546\n","train acc tensor(1., device='cuda:0')\n","test loss 0.366786764934659\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  84\n","tensor(201, device='cuda:0')\n","train loss 0.0001981035663144818\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3682111818343401\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  85\n","tensor(201, device='cuda:0')\n","train loss 0.0001939507007288436\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3696258096024394\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  86\n","tensor(201, device='cuda:0')\n","train loss 0.00018993784397025593\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3710320987738669\n","test acc tensor(0.9288, device='cuda:0')\n","\n","\n","epoch  87\n","tensor(201, device='cuda:0')\n","train loss 0.00018610280070182247\n","train acc tensor(1., device='cuda:0')\n","test loss 0.37243423657491803\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  88\n","tensor(201, device='cuda:0')\n","train loss 0.000182355777117967\n","train acc tensor(1., device='cuda:0')\n","test loss 0.37384413089603186\n","test acc tensor(0.9281, device='cuda:0')\n","\n","\n","epoch  89\n","tensor(201, device='cuda:0')\n","train loss 0.0001787797821937905\n","train acc tensor(1., device='cuda:0')\n","test loss 0.37530656391754746\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  90\n","tensor(201, device='cuda:0')\n","train loss 0.00017527875994953016\n","train acc tensor(1., device='cuda:0')\n","test loss 0.37696969462558627\n","test acc tensor(0.9275, device='cuda:0')\n","\n","\n","epoch  91\n","tensor(201, device='cuda:0')\n","train loss 0.00017192895878300382\n","train acc tensor(1., device='cuda:0')\n","test loss 0.37896046228706837\n","test acc tensor(0.9268, device='cuda:0')\n","\n","\n","epoch  92\n","tensor(201, device='cuda:0')\n","train loss 0.00016868901487517482\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3810576763935387\n","test acc tensor(0.9262, device='cuda:0')\n","\n","\n","epoch  93\n","tensor(201, device='cuda:0')\n","train loss 0.00016555588202512203\n","train acc tensor(1., device='cuda:0')\n","test loss 0.38305869325995445\n","test acc tensor(0.9262, device='cuda:0')\n","\n","\n","epoch  94\n","tensor(201, device='cuda:0')\n","train loss 0.00016251674969680607\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3849251610226929\n","test acc tensor(0.9262, device='cuda:0')\n","\n","\n","epoch  95\n","tensor(201, device='cuda:0')\n","train loss 0.00015957802073292746\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3866621283814311\n","test acc tensor(0.9262, device='cuda:0')\n","\n","\n","epoch  96\n","tensor(201, device='cuda:0')\n","train loss 0.00015672979498049244\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3882779427804053\n","test acc tensor(0.9262, device='cuda:0')\n","\n","\n","epoch  97\n","tensor(201, device='cuda:0')\n","train loss 0.0001539653106495583\n","train acc tensor(1., device='cuda:0')\n","test loss 0.38978626392781734\n","test acc tensor(0.9262, device='cuda:0')\n","\n","\n","epoch  98\n","tensor(201, device='cuda:0')\n","train loss 0.00015132389065305082\n","train acc tensor(1., device='cuda:0')\n","test loss 0.39120598835870624\n","test acc tensor(0.9256, device='cuda:0')\n","\n","\n","epoch  99\n","tensor(201, device='cuda:0')\n","train loss 0.00014872172323521227\n","train acc tensor(1., device='cuda:0')\n","test loss 0.3925548857077956\n","test acc tensor(0.9256, device='cuda:0')\n"]}]}]}