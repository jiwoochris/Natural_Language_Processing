{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_MLP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jgKuWMxk3lCZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657085661343,"user_tz":-540,"elapsed":2248,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"5630f8fe-c131-4f87-94f5-c06977800961"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#MNIST class prediction"],"metadata":{"id":"LbNjEAEBJCvx"}},{"cell_type":"markdown","source":["mnist는 숫자를 손으로 그린 그림에 대한 데이터  \n","각 그림을 보고 어떠한 숫자가 적혀있는지 classification 하는 딥러닝 모델 구축  \n","multi class classification은 강의자료의 appendix 참조  \n","\n","주요 모듈 목록  \n","**torch.utils.data.Dataset:** 데이터셋을 getitem method에서 한개씩 텐서형태로 반환하도록 구성하는 class  \n","**torch.utils.data.DataLoader:** batch processing을 위해 dataset에서 반환되는 데이터를 n개의 batch_size만큼 묶어서 반환하도록 하는 class  \n","**torch.nn.Module:** 딥러닝 모델을 구현한 class, forward method를 통해 입력된 텐서 데이터를 딥러닝 연산하여 결과 반환  \n","**torch.nn.CrossEntropyLoss:** loss 연산을 위한 class이며 backward method를 통해 gradient계산  \n","**torch.optim.Adam:** 계산된 gradient를 update해주어 feadient descent를 진행하는 class  "],"metadata":{"id":"UQc-59mMJGvr"}},{"cell_type":"markdown","source":["Dataset class구현  \n","  \n","dataset(fashion-mnist) : https://www.kaggle.com/datasets/zalando-research/fashionmnist?resource=download\n","\n","traget: label(적힌 숫자)  \n","input feature: pixel1 ~ pixel 784까지의 명도\n","train/test file이 구분되어있음  \n","\n","각 data를 torch.utils.data.Dataset을 통해 하나씩 load할 수 있는 class구현  \n","이후 torch.utils.data.DataLoader를 통해 batch개씩 변환\n","\n","torch.utils.data.Dataset의 주요 method \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(데이터셋, 데이터경로)등을 생성  \n","__ len __: 해당 클래스에서 다루는 dataset의 길이를 반환하는 함수  \n","__ getitem __(index): index에 해당하는 데이터 하나를 tensor형태로 반환하는 함수"],"metadata":{"id":"-Dui_MxkJqXY"}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/Natural_Language_Processing/2일차_배포/dataset/HousingData.csv\"\n","batch_size = 3\n","\n","#torch.utils.data.Dataset을 상속하여 Dataset class선언\n","class myDataset(Dataset):\n","  #오브젝트를 선언할때 불러오는 함수, superclass(부모클래스)의 init을 실행해 주어야함\n","  def __init__(self, df_data) -> None:\n","    super().__init__()\n","    self.df_data = df_data #self를 사용하면 class내부에서 __init__ method만이 아닌 다른 method에서도 사용가능\n","    self.data_y = df_data.loc[:,[\"MEDV\"]]\n","    self.data_x = df_data.drop([\"MEDV\"], axis=1)\n","      \n","  #list 형태의 class를 만들때 필수로 사용되는 함수, 전체 길이를 알아야 인덱싱이 가능\n","  def __len__(self):\n","    return len(self.data_y)\n","\n","  #index에 해당하는 데이터를 반환해주는 함수\n","  def __getitem__(self, index):\n","    data = torch.Tensor(self.data_x.loc[index,:])\n","    target = torch.Tensor(self.data_y.loc[index,:])\n","\n","    return data, target\n","\n","#data load후 train(400개)/test(나머지)데이터를 분할\n","data_df = pd.read_csv(data_path).dropna()\n","train_data_df = data_df.loc[:400,:].reset_index()\n","test_data_df = data_df.loc[400:,:].reset_index()\n","\n","#각 dataset을 선언\n","trainDataset = myDataset(train_data_df)\n","testDataset = myDataset(test_data_df)\n","\n","#선언된 dataset을 dataloader를 통해 batch processing\n","trainDataloader = DataLoader(trainDataset, batch_size = batch_size)\n","testDataloader = DataLoader(testDataset, batch_size = batch_size)\n","\n","#잘 작동하는지 test\n","for i in trainDataset:\n","  print(\"dataset test\")\n","  print(i)\n","  break\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"data loader test\")\n","  data = i[0]\n","  target = i[1]\n","  print(data)\n","  print(data.shape)\n","  print(target)\n","  print(target.shape)\n","  break\n"],"metadata":{"id":"SPqN84AJJGH8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657091306989,"user_tz":-540,"elapsed":267,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"2d2399e4-ec5f-4aec-f054-a6158c6de9a0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset test\n","(tensor([0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","        3.9690e+02, 4.9800e+00]), tensor([24.]))\n","data loader test\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00]])\n","torch.Size([3, 14])\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000]])\n","torch.Size([3, 1])\n"]}]},{"cell_type":"markdown","source":["Deep learning 모델 구현  \n","10개의 label의 확률을 예측해야하는 모델  \n","그렇기에 output layer의 최종 결과가 10차원의 vector가 되어야함  \n","torch.nn.Module을 이용하여 모델 구현  \n","1st hidden layer의 feature는 10개  \n","2nd hidden layer의 feature는 100개  \n","인 모델을 구현한다.  \n","\n","torch.nn.Linear: perceptron의 weighted sum과 같이 linaer regression연산을 하는 calss  \n","torch.nn.ReLU: ReLU activation function을 수행하는 class  \n","\n","torch.nn.Module의 주요 method  \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(사용할 deep learning layer, activation function, 등)등을 생성  \n","__ forward __(data): 입력받은 data를 딥러닝 모델을 통해 결과를 예측하여 반환하는 class  \n","\n","**각 딥러닝 레이어 연산 중 차원수를 확인하고 잘 맞춰줄 것**  \n","참고 document(해당 사이트의 shape를 확인하고 tensor형태 결정)  \n","nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html  \n","nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"],"metadata":{"id":"xrM7jz2NLunt"}},{"cell_type":"code","source":["from torch import nn\n","\n","#딥러닝 모델을 작성하기위한 모듈\n","class myModel(nn.Module):\n","  #오브젝트를 선언할 때 불러와지는 함수 일반적으로 이번 모델에서 사용될 각 레이어들이 포함됨\n","  def __init__(self) -> None:\n","      super().__init__()\n","\n","      #1st_hidden: 10, 2nd_hidden: 100, output: 10의 형태에 맞는 linear layer들을 선언, activation으로 Relu사용\n","    \n","  #데이터를 입력받고 딥러닝 연산후 결과를 반환하는 함수\n","  def forward(self, x):\n","\n","#작성한 모델 선언\n","model = myModel()\n"],"metadata":{"id":"1HaDnh-TMQG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["작성한 dataset과 model을 이용하여 딥러닝 프로세스 구현  \n","\n","pytorch 딥러닝 프로세스\n","1. dataset, model선언\n","2. dataset과 model을 통한 결과 예측\n","3. 예측된 결과를 통해 **loss**연산 및 **loss.backward**\n","4. **optimizer.step()**를 사용하여 graident update\n","\n","주요 오브젝트  \n","torch.nn.CrossEntropyLoss: 문장 분류를 위한 loss 수행  \n","torch.optim.Adam: Adam optimizer를 통해 gradient update를 수행하는 class\n","\n","각 오브젝트의 입력과 선언은 다음 doc 참조:  \n","CrossEntropyLoss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html  \n","Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"],"metadata":{"id":"O6yaLlOZMFOx"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","#학습을 위한 optimizer와 loss function 설정\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  #train\n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in trainDataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","\n","    #데이터 분리\n","\n","    #결과 도출\n","\n","    #loss연산\n","\n","    #loss backpropagation\n","\n","    #gradient update\n","\n","\n","\n","  #test\n","  #model이 학습되지 않는 상태로 변경\n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","\n","    #모든 test dataset에 대해서 결과연산\n","\n","    \n"],"metadata":{"id":"NNW-Nuz2M3GY"},"execution_count":null,"outputs":[]}]}