{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"price_prediction.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"od83zh3WUhw4","executionInfo":{"status":"ok","timestamp":1657088552438,"user_tz":-540,"elapsed":21060,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"240e24f1-22ca-446d-8da6-9d61e09888df"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#house price prediction in boston"],"metadata":{"id":"qCgEXYXjUPeZ"}},{"cell_type":"markdown","source":["Pytorch의 MLP를 이용한 집값예측(regression)구현  \n","  \n","주요 모듈 목록  \n","**torch.utils.data.Dataset:** 데이터셋을 getitem method에서 한개씩 텐서형태로 반환하도록 구성하는 class  \n","**torch.utils.data.DataLoader:** batch processing을 위해 dataset에서 반환되는 데이터를 n개의 batch_size만큼 묶어서 반환하도록 하는 class  \n","**torch.nn.Module:** 딥러닝 모델을 구현한 class, forward method를 통해 입력된 텐서 데이터를 딥러닝 연산하여 결과 반환  \n","**torch.nn.(loss):** loss 연산을 위한 class이며 backward method를 통해 gradient계산  \n","**torch.optim.(optimizer):** 계산된 gradient를 update해주어 feadient descent를 진행하는 class  "],"metadata":{"id":"--pvfS_VWMij"}},{"cell_type":"markdown","source":["Dataset class구현  \n","  \n","dataset(Housingdata.csv) : https://www.kaggle.com/datasets/altavish/boston-housing-dataset\n","\n","traget: MEDV(집값)  \n","input feature: MEDV를 제외한 모든 값  \n","전체 데이터 약 500개중 400개까지 train data로 사용, 나머지는 test data  \n","\n","각 data를 torch.utils.data.Dataset을 통해 하나씩 load할 수 있는 class구현  \n","이후 torch.utils.data.DataLoader를 통해 batch개씩 변환\n","\n","torch.utils.data.Dataset의 주요 method \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(데이터셋, 데이터경로)등을 생성  \n","__ len __: 해당 클래스에서 다루는 dataset의 길이를 반환하는 함수  \n","__ getitem __(index): index에 해당하는 데이터 하나를 tensor형태로 반환하는 함수"],"metadata":{"id":"_pyaoU4IYAaw"}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/2일차_배포/dataset/HousingData.csv\"\n","batch_size = 3\n","\n","#torch.utils.data.Dataset을 상속하여 Dataset class선언\n","class myDataset(Dataset):\n","  #오브젝트를 선언할때 불러오는 함수, superclass(부모클래스)의 init을 실행해 주어야함\n","  def __init__(self, df_data) -> None:\n","    super().__init__()\n","    self.df_data = df_data #self를 사용하면 class내부에서 __init__ method만이 아닌 다른 method에서도 사용가능\n","    self.data_y = df_data.loc[:,[\"MEDV\"]]\n","    self.data_x = df_data.drop([\"MEDV\"], axis=1)\n","      \n","  #list 형태의 class를 만들때 필수로 사용되는 함수, 전체 길이를 알아야 인덱싱이 가능\n","  def __len__(self):\n","    return len(self.data_y)\n","\n","  #index에 해당하는 데이터를 반환해주는 함수\n","  def __getitem__(self, index):\n","    data = torch.Tensor(self.data_x.loc[index,:])\n","    target = torch.Tensor(self.data_y.loc[index,:])\n","\n","    return data, target\n","\n","#data load후 train(400개)/test(나머지)데이터를 분할\n","data_df = pd.read_csv(data_path).dropna()\n","train_data_df = data_df.loc[:400,:].reset_index()\n","test_data_df = data_df.loc[400:,:].reset_index()\n","\n","#각 dataset을 선언\n","trainDataset = myDataset(train_data_df)\n","testDataset = myDataset(test_data_df)\n","\n","#선언된 dataset을 dataloader를 통해 batch processing\n","trainDataloader = DataLoader(trainDataset, batch_size = batch_size)\n","testDataloader = DataLoader(testDataset, batch_size = batch_size)\n","\n","#잘 작동하는지 test\n","for i in trainDataset:\n","  print(\"dataset test\")\n","  print(i)\n","  break\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"data loader test\")\n","  data = i[0]\n","  target = i[1]\n","  print(data)\n","  print(data.shape)\n","  print(target)\n","  print(target.shape)\n","  break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgwV6nIIUapc","executionInfo":{"status":"ok","timestamp":1657088706639,"user_tz":-540,"elapsed":760,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"d74ba4fc-08a5-45eb-9da8-0ca144990d39"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset test\n","(tensor([0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","        3.9690e+02, 4.9800e+00]), tensor([24.]))\n","data loader test\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00]])\n","torch.Size([3, 14])\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000]])\n","torch.Size([3, 1])\n"]}]},{"cell_type":"markdown","source":["Deep learning 모델 구현  \n","torch.nn.Module을 이용하여 모델 구현  \n","1st hidden layer의 feature는 100개  \n","2nd hidden layer의 feature는 10개  \n","인 모델을 구현한다.  \n","\n","torch.nn.Linear: perceptron의 weighted sum과 같이 linaer regression연산을 하는 calss  \n","torch.nn.ReLU: ReLU activation function을 수행하는 class  \n","\n","torch.nn.Module의 주요 method  \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(사용할 deep learning layer, activation function, 등)등을 생성  \n","__ forward __(data): 입력받은 data를 딥러닝 모델을 통해 결과를 예측하여 반환하는 class  \n","\n","**각 딥러닝 레이어 연산 중 차원수를 확인하고 잘 맞춰줄 것**  \n","참고 document(해당 사이트의 shape를 확인하고 tensor형태 결정)  \n","nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html  \n","nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"],"metadata":{"id":"n7j-zH_mfTbY"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"pxJqIF7cW-lT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657088717586,"user_tz":-540,"elapsed":344,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"2de9cbaa-61f0-4f65-e4c3-ad53e1cb0fff"},"outputs":[{"output_type":"stream","name":"stdout","text":["model test\n","input data\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00]])\n","output predict\n"," tensor([[0.],\n","        [0.],\n","        [0.]], grad_fn=<ReluBackward0>)\n","ground thruth\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000]])\n"]}],"source":["from torch import nn\n","\n","#딥러닝 모델을 작성하기위한 모듈\n","class myModel(nn.Module):\n","  #오브젝트를 선언할 때 불러와지는 함수 일반적으로 이번 모델에서 사용될 각 레이어들이 포함됨\n","  def __init__(self) -> None:\n","      super().__init__()\n","\n","      #input_feature:14,  1st_hidden: 100, 2nd_hidden: 10, output: 1의 형태에 맞는 linear layer들을 선언, activation으로 Relu사용\n","      self.linear1 = nn.Linear(14,100, bias=True)\n","      self.linear2 = nn.Linear(100,10, bias=True)\n","      self.linear3 = nn.Linear(10,1, bias=True)\n","      self.relu = nn.ReLU()\n","    \n","  #데이터를 입력받고 딥러닝 연산후 결과를 반환하는 함수\n","  def forward(self, x):\n","      #print(x.shape)\n","      x = self.linear1(x)\n","      x = self.relu(x)\n","      #print(x.shape)\n","      x = self.linear2(x)\n","      x = self.relu(x)\n","      #print(x.shape)\n","      x = self.linear3(x)\n","      x = self.relu(x)\n","      #print(x.shape)\n","\n","      return x\n","\n","#작성한 모델 선언\n","model = myModel()\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"model test\")\n","  data = i[0]\n","  target = i[1]\n","  \n","  print(\"input data\")\n","  print(data)\n","  print(\"output predict\\n\", model(data))\n","  print(\"ground thruth\")\n","  print(target)\n","  break\n"]},{"cell_type":"markdown","source":["작성한 dataset과 model을 이용하여 딥러닝 프로세스 구현  \n","\n","pytorch 딥러닝 프로세스\n","1. dataset, model선언\n","2. dataset과 model을 통한 결과 예측\n","3. 예측된 결과를 통해 **loss**연산 및 **loss.backward**\n","4. **optimizer.step()**를 사용하여 graident update\n","\n","주요 오브젝트  \n","torch.nn.MSELoss: 예측값과 정답을 통해 MSE값을 반환하는 class  \n","torch.optim.Adam: Adam optimizer를 통해 gradient update를 수행하는 class\n","\n","각 오브젝트의 입력과 선언은 다음 doc 참조:  \n","MSELoss: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html  \n","Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"],"metadata":{"id":"MvVKNpwAoymc"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","#학습을 위한 optimizer와 loss function 설정\n","optimizer = Adam(model.parameters(), lr=0.001)\n","lf = MSELoss()\n","\n","#100번의 에폭을 실행\n","for e in range(100):\n","  print(\"\\n\\nepoch \", e)\n","  epoch_loss = 0\n","  \n","  #선언한 모델 오브젝트를 학습가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in trainDataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","    data = i[0]\n","    target = i[1]\n","\n","    #결과 도출\n","    output = model(data)\n","\n","    #loss연산\n","    loss = lf(output, target)\n","    #print(loss)\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  print(\"train loss\", epoch_loss/len(trainDataloader))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in testDataloader:\n","      data = i[0]\n","      target = i[1]\n","\n","      output = model(data)\n","\n","      loss = lf(output, target)\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(testDataloader))\n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enENUuVFo4VN","executionInfo":{"status":"ok","timestamp":1657088753352,"user_tz":-540,"elapsed":31072,"user":{"displayName":"지우","userId":"14649319880676737092"}},"outputId":"dd7f2fac-9d85-4e6c-dac8-0cb11d8be7f0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","epoch  0\n","train loss 150.7045796178636\n","test loss 97.8968278339931\n","\n","\n","epoch  1\n","train loss 88.94301335641316\n","test loss 110.44436713627407\n","\n","\n","epoch  2\n","train loss 82.31035734131223\n","test loss 151.04666701384954\n","\n","\n","epoch  3\n","train loss 84.70448375542959\n","test loss 111.9696934563773\n","\n","\n","epoch  4\n","train loss 91.32892283825647\n","test loss 93.34998449257442\n","\n","\n","epoch  5\n","train loss 91.72666930116358\n","test loss 86.88042398861477\n","\n","\n","epoch  6\n","train loss 87.29349816909858\n","test loss 77.71542193208423\n","\n","\n","epoch  7\n","train loss 81.66432582537333\n","test loss 79.67355113370078\n","\n","\n","epoch  8\n","train loss 78.85525022688365\n","test loss 77.09148405279431\n","\n","\n","epoch  9\n","train loss 77.36765798500606\n","test loss 61.33498735087259\n","\n","\n","epoch  10\n","train loss 69.95015094564074\n","test loss 75.34955237592969\n","\n","\n","epoch  11\n","train loss 72.38952725785119\n","test loss 55.98701839787619\n","\n","\n","epoch  12\n","train loss 73.55289598341498\n","test loss 95.91885251658303\n","\n","\n","epoch  13\n","train loss 59.06603365114757\n","test loss 79.16885238034385\n","\n","\n","epoch  14\n","train loss 65.73192590616998\n","test loss 68.92997995444706\n","\n","\n","epoch  15\n","train loss 61.7653411377044\n","test loss 52.681184325899395\n","\n","\n","epoch  16\n","train loss 56.18222965342658\n","test loss 45.98875448533467\n","\n","\n","epoch  17\n","train loss 53.99039996067683\n","test loss 43.37178894877434\n","\n","\n","epoch  18\n","train loss 48.991905440602984\n","test loss 38.52339654735157\n","\n","\n","epoch  19\n","train loss 47.177845666522074\n","test loss 36.40331723008837\n","\n","\n","epoch  20\n","train loss 44.326745829695746\n","test loss 32.69953941021647\n","\n","\n","epoch  21\n","train loss 42.97281813337689\n","test loss 30.245802351406642\n","\n","\n","epoch  22\n","train loss 41.59640462483679\n","test loss 28.785767559494293\n","\n","\n","epoch  23\n","train loss 40.088879349827764\n","test loss 30.41117851436138\n","\n","\n","epoch  24\n","train loss 39.250069284439085\n","test loss 28.608243989092962\n","\n","\n","epoch  25\n","train loss 38.46394679234142\n","test loss 28.356256457311765\n","\n","\n","epoch  26\n","train loss 36.82535518790994\n","test loss 28.833419163312232\n","\n","\n","epoch  27\n","train loss 36.77264237162613\n","test loss 26.645256031836784\n","\n","\n","epoch  28\n","train loss 35.82387777921699\n","test loss 28.334310090967588\n","\n","\n","epoch  29\n","train loss 34.89696941645372\n","test loss 28.90975330557142\n","\n","\n","epoch  30\n","train loss 34.10702360483507\n","test loss 30.06703824017729\n","\n","\n","epoch  31\n","train loss 33.521674348413946\n","test loss 31.128338053822517\n","\n","\n","epoch  32\n","train loss 32.76337587719872\n","test loss 30.661744666951044\n","\n","\n","epoch  33\n","train loss 32.57887703932467\n","test loss 30.956904590129852\n","\n","\n","epoch  34\n","train loss 32.20305537609827\n","test loss 31.86745624244213\n","\n","\n","epoch  35\n","train loss 31.987130184542565\n","test loss 33.04503965164934\n","\n","\n","epoch  36\n","train loss 31.489642931591895\n","test loss 33.897539304835455\n","\n","\n","epoch  37\n","train loss 30.686378847417377\n","test loss 34.256937427180155\n","\n","\n","epoch  38\n","train loss 29.89087639181387\n","test loss 33.669914194515776\n","\n","\n","epoch  39\n","train loss 29.05761961319617\n","test loss 34.1519524029323\n","\n","\n","epoch  40\n","train loss 28.741068534127304\n","test loss 32.948044610874994\n","\n","\n","epoch  41\n","train loss 27.774284791378747\n","test loss 32.70593440532684\n","\n","\n","epoch  42\n","train loss 27.35846979873521\n","test loss 30.143748983740807\n","\n","\n","epoch  43\n","train loss 26.72963319676263\n","test loss 29.644169273121015\n","\n","\n","epoch  44\n","train loss 26.76901335971696\n","test loss 26.189008395586693\n","\n","\n","epoch  45\n","train loss 27.66686849707649\n","test loss 23.325734198093414\n","\n","\n","epoch  46\n","train loss 26.869754381123045\n","test loss 23.53389143518039\n","\n","\n","epoch  47\n","train loss 26.820925156843096\n","test loss 23.40315369623048\n","\n","\n","epoch  48\n","train loss 26.112995492844355\n","test loss 23.63901853987149\n","\n","\n","epoch  49\n","train loss 26.2917000412941\n","test loss 23.062342643737793\n","\n","\n","epoch  50\n","train loss 25.277779544251306\n","test loss 24.51209999833788\n","\n","\n","epoch  51\n","train loss 24.953832576672237\n","test loss 23.997555681637355\n","\n","\n","epoch  52\n","train loss 25.30348177325158\n","test loss 23.224739057677134\n","\n","\n","epoch  53\n","train loss 25.435597946814127\n","test loss 22.1615470711674\n","\n","\n","epoch  54\n","train loss 24.952609550527164\n","test loss 24.076354850615775\n","\n","\n","epoch  55\n","train loss 24.659354622307276\n","test loss 22.45193357339927\n","\n","\n","epoch  56\n","train loss 24.366099131887868\n","test loss 24.179862354482925\n","\n","\n","epoch  57\n","train loss 23.904716850746247\n","test loss 23.649960611547744\n","\n","\n","epoch  58\n","train loss 23.621450040808746\n","test loss 23.504303285053798\n","\n","\n","epoch  59\n","train loss 23.683823583008987\n","test loss 22.520519456693105\n","\n","\n","epoch  60\n","train loss 23.427648005740984\n","test loss 23.70863230739321\n","\n","\n","epoch  61\n","train loss 23.266163441325936\n","test loss 24.953428485563823\n","\n","\n","epoch  62\n","train loss 23.050774588790677\n","test loss 25.35557086765766\n","\n","\n","epoch  63\n","train loss 22.995797373602787\n","test loss 24.925026408263616\n","\n","\n","epoch  64\n","train loss 22.798536082463606\n","test loss 26.61663235085351\n","\n","\n","epoch  65\n","train loss 23.052001555299476\n","test loss 24.765619099140167\n","\n","\n","epoch  66\n","train loss 22.55521689227649\n","test loss 26.107311180659703\n","\n","\n","epoch  67\n","train loss 22.75130021809822\n","test loss 27.696723077978408\n","\n","\n","epoch  68\n","train loss 22.89162737155954\n","test loss 25.58200203520911\n","\n","\n","epoch  69\n","train loss 22.609519400625\n","test loss 31.9740709066391\n","\n","\n","epoch  70\n","train loss 22.7638788114701\n","test loss 25.62905842065811\n","\n","\n","epoch  71\n","train loss 22.66460438697111\n","test loss 25.506806228842056\n","\n","\n","epoch  72\n","train loss 22.23083402844412\n","test loss 24.25731470329421\n","\n","\n","epoch  73\n","train loss 22.48298849122865\n","test loss 25.28037327528\n","\n","\n","epoch  74\n","train loss 21.628380354600292\n","test loss 28.37504003729139\n","\n","\n","epoch  75\n","train loss 22.481693962925956\n","test loss 26.841425274099624\n","\n","\n","epoch  76\n","train loss 21.74391938291845\n","test loss 27.667668053082057\n","\n","\n","epoch  77\n","train loss 21.98645038647311\n","test loss 27.91128897666931\n","\n","\n","epoch  78\n","train loss 21.648131364725884\n","test loss 30.405402456011092\n","\n","\n","epoch  79\n","train loss 21.46585740801834\n","test loss 28.514791786670685\n","\n","\n","epoch  80\n","train loss 22.063039200646536\n","test loss 28.9759441614151\n","\n","\n","epoch  81\n","train loss 21.722040697932243\n","test loss 30.679391035011836\n","\n","\n","epoch  82\n","train loss 21.339926931687764\n","test loss 31.648272275924683\n","\n","\n","epoch  83\n","train loss 21.656817999056408\n","test loss 35.21826758555004\n","\n","\n","epoch  84\n","train loss 21.379902590456464\n","test loss 30.25809441293989\n","\n","\n","epoch  85\n","train loss 21.578652980214073\n","test loss 36.11245856114796\n","\n","\n","epoch  86\n","train loss 21.282144565951256\n","test loss 31.92011810200555\n","\n","\n","epoch  87\n","train loss 21.037539297626132\n","test loss 38.06918642350605\n","\n","\n","epoch  88\n","train loss 21.683159518525713\n","test loss 32.30482441186905\n","\n","\n","epoch  89\n","train loss 20.79263079691501\n","test loss 41.21163548742022\n","\n","\n","epoch  90\n","train loss 20.983743245261056\n","test loss 36.71409176928656\n","\n","\n","epoch  91\n","train loss 20.468235036517893\n","test loss 45.26743478860174\n","\n","\n","epoch  92\n","train loss 20.642159742826507\n","test loss 33.46548159633364\n","\n","\n","epoch  93\n","train loss 20.212733220202583\n","test loss 42.351087216820034\n","\n","\n","epoch  94\n","train loss 20.264871559256598\n","test loss 40.41539152605193\n","\n","\n","epoch  95\n","train loss 20.184237340944154\n","test loss 43.55614675794329\n","\n","\n","epoch  96\n","train loss 20.980129468157177\n","test loss 38.07661590831621\n","\n","\n","epoch  97\n","train loss 20.643609137549287\n","test loss 43.21328806025641\n","\n","\n","epoch  98\n","train loss 19.414755410665556\n","test loss 52.947926063622745\n","\n","\n","epoch  99\n","train loss 20.380474292096636\n","test loss 49.73716384598187\n"]}]}]}